{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sampling user groups from the LFM1b dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**BEFORE YOU RUN THE NOTEBOOK**:\n",
    "\n",
    "Make sure you place the unzipped [LFM1b dataset](https://drive.jku.at/ssf/s/readFile/share/1056/266403063659030189/publicLink/LFM-1b.zip) (published in [this paper](http://www.cp.jku.at/people/schedl/Research/Publications/pdf/schedl_ism_mam_2017.pdf)) in the `./data` folder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook does:\n",
    "- replicate the sampling performed in [this paper about unfairness in music recommendations](https://link.springer.com/chapter/10.1007%2F978-3-030-45442-5_5) --> \"extreme\" strategy\n",
    "- replicate the sample strategy used in [this paper about unfairness in music recommendations](https://arxiv.org/abs/1907.13286) --> \"percentile\" strategy\n",
    "- compare these two ways of creating user samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "out_folder = \"./data/user_groups\"\n",
    "\n",
    "if not os.path.exists(out_folder):\n",
    "    os.mkdir(out_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "extreme_relevant_user_ids = set()\n",
    "percentile_relevant_user_ids = set()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load LFM1b data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "users_additional = pd.read_csv(\"data/LFM-1b/LFM-1b_users_additional.txt\", sep=\"\\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "users_additional\n",
    "sorted_user_add = users_additional.sort_values(by=\"mainstreaminess_global\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample according to \"extreme\" strategy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "69165     False\n25230     False\n105083    False\n116054    False\n110784    False\n          ...  \n18190     False\n35278     False\n13605     False\n32889     False\n31726     False\nName: mainstreaminess_global, Length: 120322, dtype: bool"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_user_add[\"mainstreaminess_global\"] == sorted_user_add[\"mainstreaminess_global\"].median()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "low_main_users =  sorted_user_add.iloc[0:1000,]\n",
    "\n",
    "n = sorted_user_add.shape[0]\n",
    "lower = int((n/2) - 500)\n",
    "higher = int((n/2) + 500)\n",
    "\n",
    "medium_main_users = sorted_user_add.iloc[lower:higher,]\n",
    "high_main_users =  sorted_user_add.iloc[-1000:,]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "low_main_users[[\"user-id\", \"mainstreaminess_global\"]].to_csv(f\"{out_folder}/extreme_low_main_users.csv\")\n",
    "medium_main_users[[\"user-id\", \"mainstreaminess_global\"]].to_csv(f\"{out_folder}/extreme_medium_main_users.csv\")\n",
    "high_main_users[[\"user-id\", \"mainstreaminess_global\"]].to_csv(f\"{out_folder}/extreme_high_main_users.csv\")\n",
    "\n",
    "extreme_relevant_user_ids = extreme_relevant_user_ids.union(low_main_users[\"user-id\"])\n",
    "extreme_relevant_user_ids = extreme_relevant_user_ids.union(medium_main_users[\"user-id\"])\n",
    "extreme_relevant_user_ids = extreme_relevant_user_ids.union(high_main_users[\"user-id\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample according to \"percentile\" strategy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "twenty_percentile = users_additional[\"mainstreaminess_global\"].quantile(0.2)\n",
    "eighty_percentile = users_additional[\"mainstreaminess_global\"].quantile(0.8)\n",
    "\n",
    "low_percentile_main_users = users_additional[users_additional[\"mainstreaminess_global\"] <= twenty_percentile].sample(1000)\n",
    "medium_percentile_main_users = users_additional[np.logical_and(users_additional[\"mainstreaminess_global\"] > twenty_percentile, users_additional[\"mainstreaminess_global\"] < eighty_percentile)].sample(1000)\n",
    "high_percentile_main_users = users_additional[users_additional[\"mainstreaminess_global\"] >= eighty_percentile].sample(1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "low_percentile_main_users[[\"user-id\", \"mainstreaminess_global\"]].to_csv(f\"{out_folder}/percentile_low_main_users.csv\")\n",
    "medium_percentile_main_users[[\"user-id\", \"mainstreaminess_global\"]].to_csv(f\"{out_folder}/percentile_medium_main_users.csv\")\n",
    "high_percentile_main_users[[\"user-id\", \"mainstreaminess_global\"]].to_csv(f\"{out_folder}/percentile_high_main_users.csv\")\n",
    "\n",
    "percentile_relevant_user_ids = percentile_relevant_user_ids.union(low_percentile_main_users[\"user-id\"])\n",
    "percentile_relevant_user_ids = percentile_relevant_user_ids.union(medium_percentile_main_users[\"user-id\"])\n",
    "percentile_relevant_user_ids = percentile_relevant_user_ids.union(high_percentile_main_users[\"user-id\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Create user events files\n",
    "cols = ['user', 'artist', 'album', 'track', 'timestamp']\n",
    "full_le = dd.read_csv('data/LFM-1b/LFM-1b_LEs.txt', sep=\"\\t\", names = cols)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##############                          ] | 37% Completed |  1min 12.1s"
     ]
    }
   ],
   "source": [
    "extreme_user_le_filtering = full_le.loc[full_le[\"user\"].isin(extreme_relevant_user_ids)]\n",
    "with ProgressBar():\n",
    "    extreme_relevant_le = extreme_user_le_filtering.compute()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "percentile_user_le_filtering = full_le.loc[full_le[\"user\"].isin(percentile_relevant_user_ids)]\n",
    "with ProgressBar():\n",
    "    percentile_relevant_le = percentile_user_le_filtering.compute()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saves in a file ~ 3GB in size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{out_folder}/percentile_combined_user_events.csv\"):\n",
    "    percentile_relevant_le.to_csv(f\"{out_folder}/percentile_combined_user_events.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{out_folder}/extreme_combined_user_events.csv\"):\n",
    "    extreme_relevant_le.to_csv(f\"{out_folder}/extreme_combined_user_events.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recreate analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is mainly a copy of the `LFM_Fairness.ipynb` from the original paper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "from surprise import BaselineOnly\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import NMF\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# constants and initialization\n",
    "item_threshold = 1 # 1 means no filtering\n",
    "my_seed = 0\n",
    "rd.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "predict_col = 'artist'\n",
    "top_fraction = 0.2\n",
    "\n",
    "## CHANGE THIS ACCORDINGLY ##\n",
    "mode = \"extreme\"\n",
    "user_events_file = f'./data/user_groups/{mode}_combined_user_events.csv'\n",
    "low_user_file = f'data/user_groups/{mode}_low_main_users.csv'\n",
    "medium_user_file = f'data/user_groups/{mode}_medium_main_users.csv'\n",
    "high_user_file = f'data/user_groups/{mode}_high_main_users.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "locals()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if f'{mode}_relevant_le' in locals():\n",
    "    df_events = locals()[f\"{mode}_relevant_le\"]\n",
    "else:\n",
    "    # read user events\n",
    "    cols = ['user', 'artist', 'album', 'track', 'timestamp']\n",
    "    with ProgressBar():\n",
    "        df_events = dd.read_csv(user_events_file, names=cols).compute()\n",
    "    print('No. of user events: ' + str(len(df_events)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create user-item matrix\n",
    "df_events = df_events.groupby(['user', predict_col]).size().reset_index(name='count')\n",
    "print('No. user-item interactions: ' + str(len(df_events)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_events.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_events = df_events[df_events['count'] >= item_threshold]\n",
    "print('No. filtered user events: ' + str(len(df_events)))\n",
    "print('No. filtered items: ' + str(len(df_events[predict_col].unique())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get user distribution\n",
    "user_dist = df_events['user'].value_counts()\n",
    "num_users = len(user_dist)\n",
    "print('Mean artists per user: ' + str(user_dist.mean()))\n",
    "print('Min artists per user: ' + str(user_dist.min()))\n",
    "print('Max artists per user: ' + str(user_dist.max()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get item distribution\n",
    "item_dist = df_events[predict_col].value_counts()\n",
    "num_items = len(item_dist)\n",
    "print('No. items: ' + str(num_items))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot item distribution\n",
    "plt.figure()\n",
    "plt.plot(item_dist.values)\n",
    "plt.xlabel('Artist (log)', fontsize='14')\n",
    "plt.xticks(fontsize='13')\n",
    "plt.yticks(fontsize='13')\n",
    "plt.ylabel('Number of listeners (log)', fontsize='15')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get top items\n",
    "num_top = int(top_fraction * num_items)\n",
    "top_item_dist = item_dist[:num_top]\n",
    "print('No. top items: ' + str(len(top_item_dist)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read users\n",
    "low_users = pd.read_csv(low_user_file).set_index('user-id')\n",
    "medium_users = pd.read_csv(medium_user_file).set_index('user-id')\n",
    "high_users = pd.read_csv(high_user_file).set_index('user-id')\n",
    "no_users = len(low_users) + len(medium_users) + len(high_users)\n",
    "print('No. of users: ' + str(no_users))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get pop fractions\n",
    "pop_count = [] # number of top items per user\n",
    "user_hist = [] # user history sizes\n",
    "pop_fraq = [] # relative number of top items per user\n",
    "pop_item_fraq = [] # average popularity of items in user profiles\n",
    "low_profile_size = 0\n",
    "low_gap = 0\n",
    "medium_profile_size = 0\n",
    "medium_gap = 0\n",
    "high_profile_size = 0\n",
    "high_gap = 0\n",
    "low_count = 0\n",
    "med_count = 0\n",
    "high_count = 0\n",
    "for u, df in df_events.groupby('user'):\n",
    "    no_user_items = len(set(df[predict_col])) # profile size\n",
    "    no_user_pop_items = len(set(df[predict_col]) & set(top_item_dist.index)) # top items in profile\n",
    "    pop_count.append(no_user_pop_items)\n",
    "    user_hist.append(no_user_items)\n",
    "    pop_fraq.append(no_user_pop_items / no_user_items)\n",
    "    # get popularity (= fraction of users interacted with item) of user items and calculate average of it\n",
    "    user_pop_item_fraq = sum(item_dist[df[predict_col]] / no_users) / no_user_items\n",
    "    pop_item_fraq.append(user_pop_item_fraq)\n",
    "    if u in low_users.index: # get user group-specific values\n",
    "        low_profile_size += no_user_items\n",
    "        low_gap += user_pop_item_fraq\n",
    "        low_count += 1\n",
    "    elif u in medium_users.index:\n",
    "        medium_profile_size += no_user_items\n",
    "        medium_gap += user_pop_item_fraq\n",
    "        med_count += 1\n",
    "    else:\n",
    "        high_profile_size += no_user_items\n",
    "        high_gap += user_pop_item_fraq\n",
    "        high_count += 1\n",
    "low_profile_size /= len(low_users)\n",
    "medium_profile_size /= len(medium_users)\n",
    "high_profile_size /= len(high_users)\n",
    "low_gap /= len(low_users)\n",
    "medium_gap /= len(medium_users)\n",
    "high_gap /= len(high_users)\n",
    "print('Low count (for check): ' + str(low_count))\n",
    "print('Med count (for check): ' + str(med_count))\n",
    "print('High count (for check): ' + str(high_count))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sorted(pop_fraq))\n",
    "plt.xlabel('User', fontsize='15')\n",
    "plt.xticks(fontsize='13')\n",
    "plt.ylabel('Ratio of popular artists', fontsize='15')\n",
    "plt.yticks(fontsize='13')\n",
    "plt.axhline(y=0.8, color='black', linestyle='--', label='80% ratio of popular artists')\n",
    "plt.legend(fontsize='15')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(user_hist, pop_count)\n",
    "print('R-value: ' + str(r_value))\n",
    "line = slope * np.array(user_hist) + intercept\n",
    "plt.plot(user_hist, pop_count, 'o', user_hist, line)\n",
    "plt.xlabel('User profile size', fontsize='15')\n",
    "plt.xticks(fontsize='13')\n",
    "plt.ylabel('Number of popular artists', fontsize='15')\n",
    "plt.yticks(fontsize='13')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(user_hist, pop_item_fraq)\n",
    "print('R-value: ' + str(r_value))\n",
    "print(stats.spearmanr(user_hist, pop_item_fraq))\n",
    "line = slope * np.array(user_hist) + intercept\n",
    "plt.plot(user_hist, pop_item_fraq, 'o', user_hist, line)\n",
    "plt.xlabel('User profile size', fontsize='15')\n",
    "plt.ylabel('Average popularity of artists', fontsize='15')\n",
    "plt.xticks(fontsize='13')\n",
    "plt.yticks(fontsize='13')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Average LowMS profile size: ' + str(low_profile_size))\n",
    "print('Average MedMS profile size: ' + str(medium_profile_size))\n",
    "print('Average HighMS profile size: ' + str(high_profile_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recommendations using Surprise package"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaled_df_events = pd.DataFrame()\n",
    "for user_id, group in df_events.groupby('user'):\n",
    "    min_rating = group['count'].min()\n",
    "    max_rating = group['count'].max()\n",
    "    scaler = MinMaxScaler(feature_range=(1, 1000))\n",
    "    scaled_ratings = scaler.fit_transform(group['count'].values.reshape(-1, 1).astype(float))\n",
    "    new_rows = group.copy()\n",
    "    new_rows['count'] = scaled_ratings\n",
    "    scaled_df_events = scaled_df_events.append(new_rows)\n",
    "\n",
    "scaled_df_events.head()\n",
    "#scaled_df_events = scaled_df_events.set_index('user') # needed for new python/surprise version"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_events = scaled_df_events\n",
    "print('Min rating: ' + str(df_events['count'].min()))\n",
    "print('Max rating: ' + str(df_events['count'].max()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(df_events['count'].min(), df_events['count'].max()))\n",
    "df_events.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df_events, reader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size = 0.2, random_state = my_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_top_n_random(testset, n=10):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r in testset:\n",
    "        if len(top_n[uid]) == 0:\n",
    "            for i in range(0, 10):\n",
    "                top_n[uid].append((rd.choice(item_dist.index), i))\n",
    "    return top_n\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_top_n_mp(testset, n=10):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r in testset:\n",
    "        if len(top_n[uid]) == 0:\n",
    "            for iid, count in item_dist[:n].items():\n",
    "                top_n[uid].append((iid, count))\n",
    "    return top_n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_mae_of_groups(predictions):\n",
    "    print('All: ')\n",
    "    accuracy.mae(predictions)\n",
    "    low_predictions = []\n",
    "    med_predictions = []\n",
    "    high_predictions = []\n",
    "    for uid, iid, true_r, est, details in predictions:\n",
    "        prediction = [(uid, iid, true_r, est, details)]\n",
    "        if uid in low_users.index:\n",
    "            low_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "        elif uid in medium_users.index:\n",
    "            med_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "        else:\n",
    "            high_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "    print('LowMS: ' + str(np.mean(low_predictions)))\n",
    "    print('MedMS: ' + str(np.mean(med_predictions)))\n",
    "    print('HighMS: ' + str(np.mean(high_predictions)))\n",
    "    print(stats.ttest_ind(low_predictions, high_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create item dataframe with normalized item counts\n",
    "df_item_dist = pd.DataFrame(item_dist)\n",
    "df_item_dist.columns = ['count']\n",
    "df_item_dist['count'] /= no_users"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sim_users = {'name': 'cosine', 'user_based': True}  # compute cosine similarities between users\n",
    "algos = [] # Random and MostPopular is calculated by default\n",
    "algos.append(None)#Random())\n",
    "algos.append(None)#MostPopular())\n",
    "algos.append(BaselineOnly())\n",
    "algos.append(KNNBasic(sim_options = sim_users, k=40))\n",
    "algos.append(KNNWithMeans(sim_options = sim_users, k=40))\n",
    "algos.append(NMF(n_factors = 15))\n",
    "algo_names = ['Random',\n",
    "              'MostPopular',\n",
    "              'UserItemAvg',\n",
    "              'UserKNN',\n",
    "              'UserKNNAvg',\n",
    "              'NMF']\n",
    "\n",
    "i = 0\n",
    "low_rec_gap_list = [] # one entry per algorithmus\n",
    "medium_rec_gap_list = []\n",
    "high_rec_gap_list = []\n",
    "for i in range(0, len(algo_names)):\n",
    "    df_item_dist[algo_names[i]] = 0\n",
    "    low_rec_gap = 0\n",
    "    medium_rec_gap = 0\n",
    "    high_rec_gap = 0\n",
    "\n",
    "    # get accuracy for personalized approaches\n",
    "    if algo_names[i] is not 'Random' and algo_names[i] is not 'MostPopular':\n",
    "        algos[i].fit(trainset)\n",
    "        predictions = algos[i].test(testset)\n",
    "        print(algo_names[i])\n",
    "        get_mae_of_groups(predictions)\n",
    "\n",
    "    # get top-n items and calculate gaps for all algorithms\n",
    "    if algo_names[i] == 'Random':\n",
    "        top_n = get_top_n_random(testset, n=10)\n",
    "    elif algo_names[i] == 'MostPopular':\n",
    "        top_n = get_top_n_mp(testset, n=10)\n",
    "    else:\n",
    "        top_n = get_top_n(predictions, n=10)\n",
    "    low_count = 0\n",
    "    med_count = 0\n",
    "    high_count = 0\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        iid_list = []\n",
    "        for (iid, _) in user_ratings:\n",
    "            df_item_dist.loc[iid, algo_names[i]] += 1\n",
    "            iid_list.append(iid)\n",
    "        gap = sum(item_dist[iid_list] / no_users) / len(iid_list)\n",
    "        if uid in low_users.index:\n",
    "            low_rec_gap += gap\n",
    "            low_count += 1\n",
    "        elif uid in medium_users.index:\n",
    "            medium_rec_gap += gap\n",
    "            med_count += 1\n",
    "        elif uid in high_users.index:\n",
    "            high_rec_gap += gap\n",
    "            high_count += 1\n",
    "    low_rec_gap_list.append(low_rec_gap / low_count)\n",
    "    medium_rec_gap_list.append(medium_rec_gap / med_count)\n",
    "    high_rec_gap_list.append(high_rec_gap / high_count)\n",
    "    i += 1 # next algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(0, len(algo_names)):\n",
    "    plt.figure()\n",
    "    x = df_item_dist['count']\n",
    "    y = df_item_dist[algo_names[i]]\n",
    "    #slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "    #line = slope * np.array(x) + intercept\n",
    "    #print(r_value)\n",
    "    if algo_names[i] is not 'Random' and algo_names[i] is not 'MostPopular':\n",
    "        plt.gca().set_ylim(0, 300)\n",
    "    plt.plot(x, y, 'o')#, x, line)\n",
    "    plt.xlabel('Artist popularity', fontsize='15')\n",
    "    plt.ylabel('Recommendation frequency', fontsize='15')\n",
    "    plt.xticks(fontsize='13')\n",
    "    plt.yticks(fontsize='13')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "low_gap_vals = []\n",
    "medium_gap_vals = []\n",
    "high_gap_vals = []\n",
    "\n",
    "for i in range(0, len(algos)):\n",
    "    low_gap_vals.append((low_rec_gap_list[i] - low_gap) / low_gap * 100)\n",
    "    medium_gap_vals.append((medium_rec_gap_list[i] - medium_gap) / medium_gap * 100)\n",
    "    high_gap_vals.append((high_rec_gap_list[i] - high_gap) / high_gap * 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set width of bar\n",
    "barWidth = 0.15\n",
    "\n",
    "# set height of bar\n",
    "bars1 = [low_gap_vals[0], medium_gap_vals[0], high_gap_vals[0]]\n",
    "bars2 = [low_gap_vals[1], medium_gap_vals[1], high_gap_vals[1]]\n",
    "bars3 = [low_gap_vals[2], medium_gap_vals[2], high_gap_vals[2]]\n",
    "bars4 = [low_gap_vals[3], medium_gap_vals[3], high_gap_vals[3]]\n",
    "bars5 = [low_gap_vals[4], medium_gap_vals[4], high_gap_vals[4]]\n",
    "bars6 = [low_gap_vals[5], medium_gap_vals[5], high_gap_vals[5]]\n",
    "\n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "r5 = [x + barWidth for x in r4]\n",
    "r6 = [x + barWidth for x in r5]\n",
    "\n",
    "# Make the plot\n",
    "plt.bar(r1, bars1, width=barWidth, label='Random')\n",
    "plt.bar(r2, bars2, width=barWidth, label='MostPopular')\n",
    "plt.bar(r3, bars3, width=barWidth, label='UserItemAvg')\n",
    "plt.bar(r4, bars4, width=barWidth, label='UserKNN')\n",
    "plt.bar(r5, bars5, width=barWidth, label='UserKNNAvg')\n",
    "plt.bar(r6, bars6, width=barWidth, label='NMF')\n",
    "\n",
    "# Add xticks on the middle of the group bars + show legend\n",
    "plt.xlabel('User group', fontsize='15')\n",
    "plt.ylabel('% $\\Delta$ GAP', fontsize='15')\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], ['LowMS', 'MedMS', 'HighMS'], fontsize='13')\n",
    "plt.yticks(fontsize='13')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0., framealpha=1, fontsize='15')\n",
    "#plt.savefig('data/ECIR/gap_analysis.png', dpi=300, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}