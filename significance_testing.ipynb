{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "from surprise import AlgoBase\n",
    "from surprise import BaselineOnly\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import NMF\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and initialization\n",
    "item_threshold = 1 # 1 means no filtering\n",
    "my_seed = 0\n",
    "rd.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "predict_col = 'artist'\n",
    "top_fraction = 0.2\n",
    "user_events_file = 'data/user_events.csv'\n",
    "low_user_file = 'data/low_main_users.csv'\n",
    "medium_user_file = 'data/medium_main_users.csv'\n",
    "high_user_file = 'data/high_main_users.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of user events: 28718087\n"
     ]
    }
   ],
   "source": [
    "# read user events\n",
    "cols = ['user', 'artist', 'album', 'track', 'timestamp']\n",
    "df_events = pd.read_csv(user_events_file, sep='\\t', names=cols)\n",
    "print('No. of user events: ' + str(len(df_events)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. user-item interactions: 1755361\n"
     ]
    }
   ],
   "source": [
    "# create user-item matrix\n",
    "df_events = df_events.groupby(['user', predict_col]).size().reset_index(name='count')\n",
    "print('No. user-item interactions: ' + str(len(df_events)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>artist</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1021445</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021445</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1021445</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1021445</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021445</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user  artist  count\n",
       "0  1021445      12     43\n",
       "1  1021445      16      1\n",
       "2  1021445      28      7\n",
       "3  1021445      29      1\n",
       "4  1021445      46      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. filtered user events: 1755361\n",
      "No. filtered items: 352805\n"
     ]
    }
   ],
   "source": [
    "df_events = df_events[df_events['count'] >= item_threshold]\n",
    "print('No. filtered user events: ' + str(len(df_events)))\n",
    "print('No. filtered items: ' + str(len(df_events[predict_col].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean artists per user: 585.1203333333333\n",
      "Min artists per user: 18\n",
      "Max artists per user: 4011\n"
     ]
    }
   ],
   "source": [
    "# get user distribution\n",
    "user_dist = df_events['user'].value_counts()\n",
    "num_users = len(user_dist)\n",
    "print('Mean artists per user: ' + str(user_dist.mean()))\n",
    "print('Min artists per user: ' + str(user_dist.min()))\n",
    "print('Max artists per user: ' + str(user_dist.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. items: 352805\n"
     ]
    }
   ],
   "source": [
    "# get item distribution\n",
    "item_dist = df_events[predict_col].value_counts()\n",
    "num_items = len(item_dist)\n",
    "print('No. items: ' + str(num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. top items: 70561\n"
     ]
    }
   ],
   "source": [
    "# get top items\n",
    "num_top = int(top_fraction * num_items)\n",
    "top_item_dist = item_dist[:num_top]\n",
    "print('No. top items: ' + str(len(top_item_dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of users: 3000\n"
     ]
    }
   ],
   "source": [
    "# read users\n",
    "low_users = pd.read_csv(low_user_file, sep=',').set_index('user_id')\n",
    "medium_users = pd.read_csv(medium_user_file, sep=',').set_index('user_id')\n",
    "high_users = pd.read_csv(high_user_file, sep=',').set_index('user_id')\n",
    "no_users = len(low_users) + len(medium_users) + len(high_users)\n",
    "print('No. of users: ' + str(no_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low count (for check): 1000\n",
      "Med count (for check): 1000\n",
      "High count (for check): 1000\n"
     ]
    }
   ],
   "source": [
    "# get pop fractions\n",
    "pop_count = [] # number of top items per user\n",
    "user_hist = [] # user history sizes\n",
    "pop_fraq = [] # relative number of top items per user\n",
    "pop_item_fraq = [] # average popularity of items in user profiles\n",
    "low_profile_size = 0\n",
    "low_gap = 0\n",
    "medium_profile_size = 0\n",
    "medium_gap = 0\n",
    "high_profile_size = 0\n",
    "high_gap = 0\n",
    "low_count = 0\n",
    "med_count = 0\n",
    "high_count = 0\n",
    "for u, df in df_events.groupby('user'):\n",
    "    no_user_items = len(set(df[predict_col])) # profile size\n",
    "    no_user_pop_items = len(set(df[predict_col]) & set(top_item_dist.index)) # top items in profile\n",
    "    pop_count.append(no_user_pop_items)\n",
    "    user_hist.append(no_user_items)\n",
    "    pop_fraq.append(no_user_pop_items / no_user_items)\n",
    "    # get popularity (= fraction of users interacted with item) of user items and calculate average of it\n",
    "    user_pop_item_fraq = sum(item_dist[df[predict_col]] / no_users) / no_user_items\n",
    "    pop_item_fraq.append(user_pop_item_fraq)\n",
    "    if u in low_users.index: # get user group-specific values\n",
    "        low_profile_size += no_user_items\n",
    "        low_gap += user_pop_item_fraq\n",
    "        low_count += 1\n",
    "    elif u in medium_users.index:\n",
    "        medium_profile_size += no_user_items\n",
    "        medium_gap += user_pop_item_fraq\n",
    "        med_count += 1\n",
    "    else:\n",
    "        high_profile_size += no_user_items\n",
    "        high_gap += user_pop_item_fraq\n",
    "        high_count += 1\n",
    "low_profile_size /= len(low_users)\n",
    "medium_profile_size /= len(medium_users)\n",
    "high_profile_size /= len(high_users)\n",
    "low_gap /= len(low_users)\n",
    "medium_gap /= len(medium_users)\n",
    "high_gap /= len(high_users)\n",
    "print('Low count (for check): ' + str(low_count))\n",
    "print('Med count (for check): ' + str(med_count))\n",
    "print('High count (for check): ' + str(high_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>artist</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1021445</td>\n",
       "      <td>12</td>\n",
       "      <td>184.222707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021445</td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1021445</td>\n",
       "      <td>28</td>\n",
       "      <td>27.174672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1021445</td>\n",
       "      <td>29</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021445</td>\n",
       "      <td>46</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user  artist       count\n",
       "0  1021445      12  184.222707\n",
       "1  1021445      16    1.000000\n",
       "2  1021445      28   27.174672\n",
       "3  1021445      29    1.000000\n",
       "4  1021445      46    1.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df_events = pd.DataFrame()\n",
    "for user_id, group in df_events.groupby('user'):\n",
    "    min_rating = group['count'].min()\n",
    "    max_rating = group['count'].max()\n",
    "    scaler = MinMaxScaler(feature_range=(1, 1000))\n",
    "    scaled_ratings = scaler.fit_transform(group['count'].values.reshape(-1, 1).astype(float))\n",
    "    new_rows = group.copy()\n",
    "    new_rows['count'] = scaled_ratings\n",
    "    scaled_df_events = scaled_df_events.append(new_rows)\n",
    "\n",
    "scaled_df_events.head()\n",
    "#scaled_df_events = scaled_df_events.set_index('user') # needed for new python/surprise version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min rating: 1.0\n",
      "Max rating: 1000.0000000000001\n"
     ]
    }
   ],
   "source": [
    "df_events = scaled_df_events\n",
    "print('Min rating: ' + str(df_events['count'].min()))\n",
    "print('Max rating: ' + str(df_events['count'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>artist</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1021445</td>\n",
       "      <td>12</td>\n",
       "      <td>184.222707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021445</td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1021445</td>\n",
       "      <td>28</td>\n",
       "      <td>27.174672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1021445</td>\n",
       "      <td>29</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021445</td>\n",
       "      <td>46</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user  artist       count\n",
       "0  1021445      12  184.222707\n",
       "1  1021445      16    1.000000\n",
       "2  1021445      28   27.174672\n",
       "3  1021445      29    1.000000\n",
       "4  1021445      46    1.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(df_events['count'].min(), df_events['count'].max()))\n",
    "df_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df_events, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size = 0.2, random_state = my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_random(testset, n=10):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r in testset:\n",
    "        if len(top_n[uid]) == 0:\n",
    "            for i in range(0, 10):\n",
    "                top_n[uid].append((rd.choice(item_dist.index), i))\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_mp(testset, n=10):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r in testset:\n",
    "        if len(top_n[uid]) == 0:\n",
    "            for iid, count in item_dist[:n].items():\n",
    "                top_n[uid].append((iid, count))\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae_of_groups(predictions):\n",
    "    print('All: ')\n",
    "    accuracy.mae(predictions)\n",
    "    low_predictions = []\n",
    "    med_predictions = []\n",
    "    high_predictions = []\n",
    "    for uid, iid, true_r, est, details in predictions:\n",
    "        prediction = [(uid, iid, true_r, est, details)]\n",
    "        if uid in low_users.index:\n",
    "            low_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "        elif uid in medium_users.index:\n",
    "            med_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "        else:\n",
    "            high_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "    print('LowMS: ' + str(np.mean(low_predictions)))\n",
    "    print('MedMS: ' + str(np.mean(med_predictions)))\n",
    "    print('HighMS: ' + str(np.mean(high_predictions)))\n",
    "    print(stats.ttest_ind(low_predictions, high_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create item dataframe with normalized item counts\n",
    "df_item_dist = pd.DataFrame(item_dist)\n",
    "df_item_dist.columns = ['count']\n",
    "df_item_dist['count'] /= no_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:27: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:27: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:27: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:27: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\PREINS~1\\AppData\\Local\\Temp/ipykernel_18016/3223305696.py:27: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if algo_names[i] is not 'Random' and algo_names[i] is not 'MostPopular':\n",
      "C:\\Users\\PREINS~1\\AppData\\Local\\Temp/ipykernel_18016/3223305696.py:27: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if algo_names[i] is not 'Random' and algo_names[i] is not 'MostPopular':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "UserItemAvg\n",
      "All: \n",
      "MAE:  38.5612\n",
      "LowMS: 42.94802225638076\n",
      "MedMS: 33.90013072887102\n",
      "HighMS: 40.68639747115602\n",
      "Ttest_indResult(statistic=6.410087809223839, pvalue=1.4574568991380452e-10)\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PREINS~1\\AppData\\Local\\Temp/ipykernel_18016/3223305696.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0malgo_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;34m'Random'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0malgo_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;34m'MostPopular'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0malgos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgo_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mget_mae_of_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python\\Python38\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, testset, verbose)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# The ratings are translated back to their original scale.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         predictions = [self.predict(uid,\n\u001b[0m\u001b[0;32m    165\u001b[0m                                     \u001b[0miid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                                     \u001b[0mr_ui_trans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python\\Python38\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# The ratings are translated back to their original scale.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         predictions = [self.predict(uid,\n\u001b[0m\u001b[0;32m    165\u001b[0m                                     \u001b[0miid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                                     \u001b[0mr_ui_trans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python\\Python38\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, uid, iid, r_ui, clip, verbose)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mdetails\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miuid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miiid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;31m# If the details dict was also returned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python\\Python38\\lib\\site-packages\\surprise\\prediction_algorithms\\knns.py\u001b[0m in \u001b[0;36mestimate\u001b[1;34m(self, u, i)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mneighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mk_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheapq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# compute weighted average\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python\\Python38\\lib\\heapq.py\u001b[0m in \u001b[0;36mnlargest\u001b[1;34m(n, iterable, key)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[1;31m# General case, slowest method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python\\Python38\\lib\\heapq.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[1;31m# General case, slowest method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sim_users = {'name': 'cosine', 'user_based': True}  # compute cosine similarities between users\n",
    "algos = [] # Random and MostPopular is calculated by default\n",
    "algos.append(None)#Random())\n",
    "algos.append(None)#MostPopular())\n",
    "algos.append(BaselineOnly())\n",
    "algos.append(KNNBasic(sim_options = sim_users, k=40))\n",
    "algos.append(KNNWithMeans(sim_options = sim_users, k=40))\n",
    "algos.append(NMF(n_factors = 15))\n",
    "algo_names = ['Random',\n",
    "              'MostPopular',\n",
    "              'UserItemAvg',\n",
    "              'UserKNN',\n",
    "              'UserKNNAvg',\n",
    "              'NMF']\n",
    "\n",
    "i = 0\n",
    "low_rec_gap_list = [] # one entry per algorithmus\n",
    "medium_rec_gap_list = []\n",
    "high_rec_gap_list = []\n",
    "for i in range(0, len(algo_names)):\n",
    "    df_item_dist[algo_names[i]] = 0\n",
    "    low_rec_gap = 0\n",
    "    medium_rec_gap = 0\n",
    "    high_rec_gap = 0\n",
    "\n",
    "    # get accuracy for personalized approaches\n",
    "    if algo_names[i] is not 'Random' and algo_names[i] is not 'MostPopular':\n",
    "        algos[i].fit(trainset)\n",
    "        predictions = algos[i].test(testset)\n",
    "        print(algo_names[i])\n",
    "        get_mae_of_groups(predictions)\n",
    "\n",
    "    # get top-n items and calculate gaps for all algorithms\n",
    "    if algo_names[i] == 'Random':\n",
    "        top_n = get_top_n_random(testset, n=10)\n",
    "    elif algo_names[i] == 'MostPopular':\n",
    "        top_n = get_top_n_mp(testset, n=10)\n",
    "    else:\n",
    "        top_n = get_top_n(predictions, n=10)\n",
    "    low_count = 0\n",
    "    med_count = 0\n",
    "    high_count = 0\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        iid_list = []\n",
    "        for (iid, _) in user_ratings:\n",
    "            df_item_dist.loc[iid, algo_names[i]] += 1\n",
    "            iid_list.append(iid)\n",
    "        gap = sum(item_dist[iid_list] / no_users) / len(iid_list)\n",
    "        if uid in low_users.index:\n",
    "            low_rec_gap += gap\n",
    "            low_count += 1\n",
    "        elif uid in medium_users.index:\n",
    "            medium_rec_gap += gap\n",
    "            med_count += 1\n",
    "        elif uid in high_users.index:\n",
    "            high_rec_gap += gap\n",
    "            high_count += 1\n",
    "    low_rec_gap_list.append(low_rec_gap / low_count)\n",
    "    medium_rec_gap_list.append(medium_rec_gap / med_count)\n",
    "    high_rec_gap_list.append(high_rec_gap / high_count)\n",
    "    i += 1 # next algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(algo_names)):\n",
    "    plt.figure()\n",
    "    x = df_item_dist['count']\n",
    "    y = df_item_dist[algo_names[i]]\n",
    "    #slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "    #line = slope * np.array(x) + intercept\n",
    "    #print(r_value)\n",
    "    if algo_names[i] is not 'Random' and algo_names[i] is not 'MostPopular':\n",
    "        plt.gca().set_ylim(0, 300)\n",
    "    plt.plot(x, y, 'o')#, x, line)\n",
    "    plt.xlabel('Artist popularity', fontsize='15')\n",
    "    plt.ylabel('Recommendation frequency', fontsize='15')\n",
    "    plt.xticks(fontsize='13')\n",
    "    plt.yticks(fontsize='13')\n",
    "    #plt.savefig('data/ECIR/rec_' + algo_names[i] + '.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_gap_vals = []\n",
    "medium_gap_vals = []\n",
    "high_gap_vals = []\n",
    "\n",
    "for i in range(0, len(algos)):\n",
    "    low_gap_vals.append((low_rec_gap_list[i] - low_gap) / low_gap * 100)\n",
    "    medium_gap_vals.append((medium_rec_gap_list[i] - medium_gap) / medium_gap * 100)\n",
    "    high_gap_vals.append((high_rec_gap_list[i] - high_gap) / high_gap * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set width of bar\n",
    "barWidth = 0.15\n",
    "\n",
    "# set height of bar\n",
    "bars1 = [low_gap_vals[0], medium_gap_vals[0], high_gap_vals[0]]\n",
    "bars2 = [low_gap_vals[1], medium_gap_vals[1], high_gap_vals[1]]\n",
    "bars3 = [low_gap_vals[2], medium_gap_vals[2], high_gap_vals[2]]\n",
    "bars4 = [low_gap_vals[3], medium_gap_vals[3], high_gap_vals[3]]\n",
    "bars5 = [low_gap_vals[4], medium_gap_vals[4], high_gap_vals[4]]\n",
    "bars6 = [low_gap_vals[5], medium_gap_vals[5], high_gap_vals[5]]\n",
    "\n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    "r5 = [x + barWidth for x in r4]\n",
    "r6 = [x + barWidth for x in r5]\n",
    "\n",
    "# Make the plot\n",
    "plt.bar(r1, bars1, width=barWidth, label='Random')\n",
    "plt.bar(r2, bars2, width=barWidth, label='MostPopular')\n",
    "plt.bar(r3, bars3, width=barWidth, label='UserItemAvg')\n",
    "plt.bar(r4, bars4, width=barWidth, label='UserKNN')\n",
    "plt.bar(r5, bars5, width=barWidth, label='UserKNNAvg')\n",
    "plt.bar(r6, bars6, width=barWidth, label='NMF')\n",
    "\n",
    "# Add xticks on the middle of the group bars + show legend\n",
    "plt.xlabel('User group', fontsize='15')\n",
    "plt.ylabel('% $\\Delta$ GAP', fontsize='15')\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], ['LowMS', 'MedMS', 'HighMS'], fontsize='13')\n",
    "plt.yticks(fontsize='13')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0., framealpha=1, fontsize='15')\n",
    "#plt.savefig('data/ECIR/gap_analysis.png', dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c627be1547c7c874688ddee1aaaa67b367275f1752d282bff5eb427acd241334"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
