{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "from surprise import AlgoBase\n",
    "from surprise import BaselineOnly\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import NMF\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and initialization\n",
    "item_threshold = 1 # 1 means no filtering\n",
    "my_seed = 0\n",
    "rd.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "predict_col = 'artist'\n",
    "top_fraction = 0.2\n",
    "user_events_file = 'data/user_events.csv'\n",
    "low_user_file = 'data/low_main_users.csv'\n",
    "medium_user_file = 'data/medium_main_users.csv'\n",
    "high_user_file = 'data/high_main_users.csv'\n",
    "performance_data_file = 'data/performance_data.csv'\n",
    "low_mae_data_file = 'data/low_mae_data.csv'\n",
    "med_mae_data_file = 'data/med_mae_data.csv'\n",
    "high_mae_data_file = 'data/high_mae_data.csv'\n",
    "performance_data_cv_file = 'data/performance_data_cv.csv'\n",
    "low_mae_data_cv_file = 'data/low_mae_data_cv.csv'\n",
    "med_mae_data_cv_file = 'data/med_mae_data_cv.csv'\n",
    "high_mae_data_cv_file = 'data/high_mae_data_cv.csv'\n",
    "two_sample_ttest_data_file = 'data/two_sample_ttest_data.csv'\n",
    "two_sample_ttest_data_cv_file = 'data/two_sample_ttest_data_cv.csv'\n",
    "paired_sample_ttest_data_file = 'data/paired_sample_ttest_data.csv'\n",
    "paired_sample_ttest_data_cv_file = 'data/paired_sample_ttest_data_cv.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of user events: 28718087\n"
     ]
    }
   ],
   "source": [
    "# read user events\n",
    "cols = ['user', 'artist', 'album', 'track', 'timestamp']\n",
    "df_events = pd.read_csv(user_events_file, sep='\\t', names=cols)\n",
    "print('No. of user events: ' + str(len(df_events)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. user-item interactions: 1755361\n"
     ]
    }
   ],
   "source": [
    "# create user-item matrix\n",
    "df_events = df_events.groupby(['user', predict_col]).size().reset_index(name='count')\n",
    "print('No. user-item interactions: ' + str(len(df_events)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>artist</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1021445</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021445</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1021445</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1021445</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021445</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user  artist  count\n",
       "0  1021445      12     43\n",
       "1  1021445      16      1\n",
       "2  1021445      28      7\n",
       "3  1021445      29      1\n",
       "4  1021445      46      1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. filtered user events: 1755361\n",
      "No. filtered items: 352805\n"
     ]
    }
   ],
   "source": [
    "df_events = df_events[df_events['count'] >= item_threshold]\n",
    "print('No. filtered user events: ' + str(len(df_events)))\n",
    "print('No. filtered items: ' + str(len(df_events[predict_col].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean artists per user: 585.1203333333333\n",
      "Min artists per user: 18\n",
      "Max artists per user: 4011\n"
     ]
    }
   ],
   "source": [
    "# get user distribution\n",
    "user_dist = df_events['user'].value_counts()\n",
    "num_users = len(user_dist)\n",
    "print('Mean artists per user: ' + str(user_dist.mean()))\n",
    "print('Min artists per user: ' + str(user_dist.min()))\n",
    "print('Max artists per user: ' + str(user_dist.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. items: 352805\n"
     ]
    }
   ],
   "source": [
    "# get item distribution\n",
    "item_dist = df_events[predict_col].value_counts()\n",
    "num_items = len(item_dist)\n",
    "print('No. items: ' + str(num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. top items: 70561\n"
     ]
    }
   ],
   "source": [
    "# get top items\n",
    "num_top = int(top_fraction * num_items)\n",
    "top_item_dist = item_dist[:num_top]\n",
    "print('No. top items: ' + str(len(top_item_dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of users: 3000\n"
     ]
    }
   ],
   "source": [
    "# read users\n",
    "low_users = pd.read_csv(low_user_file, sep=',').set_index('user_id')\n",
    "medium_users = pd.read_csv(medium_user_file, sep=',').set_index('user_id')\n",
    "high_users = pd.read_csv(high_user_file, sep=',').set_index('user_id')\n",
    "no_users = len(low_users) + len(medium_users) + len(high_users)\n",
    "print('No. of users: ' + str(no_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low count (for check): 1000\n",
      "Med count (for check): 1000\n",
      "High count (for check): 1000\n"
     ]
    }
   ],
   "source": [
    "# get pop fractions\n",
    "pop_count = [] # number of top items per user\n",
    "user_hist = [] # user history sizes\n",
    "pop_fraq = [] # relative number of top items per user\n",
    "pop_item_fraq = [] # average popularity of items in user profiles\n",
    "low_profile_size = 0\n",
    "low_gap = 0\n",
    "medium_profile_size = 0\n",
    "medium_gap = 0\n",
    "high_profile_size = 0\n",
    "high_gap = 0\n",
    "low_count = 0\n",
    "med_count = 0\n",
    "high_count = 0\n",
    "for u, df in df_events.groupby('user'):\n",
    "    no_user_items = len(set(df[predict_col])) # profile size\n",
    "    no_user_pop_items = len(set(df[predict_col]) & set(top_item_dist.index)) # top items in profile\n",
    "    pop_count.append(no_user_pop_items)\n",
    "    user_hist.append(no_user_items)\n",
    "    pop_fraq.append(no_user_pop_items / no_user_items)\n",
    "    # get popularity (= fraction of users interacted with item) of user items and calculate average of it\n",
    "    user_pop_item_fraq = sum(item_dist[df[predict_col]] / no_users) / no_user_items\n",
    "    pop_item_fraq.append(user_pop_item_fraq)\n",
    "    if u in low_users.index: # get user group-specific values\n",
    "        low_profile_size += no_user_items\n",
    "        low_gap += user_pop_item_fraq\n",
    "        low_count += 1\n",
    "    elif u in medium_users.index:\n",
    "        medium_profile_size += no_user_items\n",
    "        medium_gap += user_pop_item_fraq\n",
    "        med_count += 1\n",
    "    else:\n",
    "        high_profile_size += no_user_items\n",
    "        high_gap += user_pop_item_fraq\n",
    "        high_count += 1\n",
    "low_profile_size /= len(low_users)\n",
    "medium_profile_size /= len(medium_users)\n",
    "high_profile_size /= len(high_users)\n",
    "low_gap /= len(low_users)\n",
    "medium_gap /= len(medium_users)\n",
    "high_gap /= len(high_users)\n",
    "print('Low count (for check): ' + str(low_count))\n",
    "print('Med count (for check): ' + str(med_count))\n",
    "print('High count (for check): ' + str(high_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>artist</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1021445</td>\n",
       "      <td>12</td>\n",
       "      <td>184.222707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021445</td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1021445</td>\n",
       "      <td>28</td>\n",
       "      <td>27.174672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1021445</td>\n",
       "      <td>29</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021445</td>\n",
       "      <td>46</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user  artist       count\n",
       "0  1021445      12  184.222707\n",
       "1  1021445      16    1.000000\n",
       "2  1021445      28   27.174672\n",
       "3  1021445      29    1.000000\n",
       "4  1021445      46    1.000000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df_events = pd.DataFrame()\n",
    "for user_id, group in df_events.groupby('user'):\n",
    "    min_rating = group['count'].min()\n",
    "    max_rating = group['count'].max()\n",
    "    scaler = MinMaxScaler(feature_range=(1, 1000))\n",
    "    scaled_ratings = scaler.fit_transform(group['count'].values.reshape(-1, 1).astype(float))\n",
    "    new_rows = group.copy()\n",
    "    new_rows['count'] = scaled_ratings\n",
    "    scaled_df_events = scaled_df_events.append(new_rows)\n",
    "\n",
    "scaled_df_events.head()\n",
    "#scaled_df_events = scaled_df_events.set_index('user') # needed for new python/surprise version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min rating: 1.0\n",
      "Max rating: 1000.0000000000001\n"
     ]
    }
   ],
   "source": [
    "df_events = scaled_df_events\n",
    "print('Min rating: ' + str(df_events['count'].min()))\n",
    "print('Max rating: ' + str(df_events['count'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>artist</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1021445</td>\n",
       "      <td>12</td>\n",
       "      <td>184.222707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021445</td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1021445</td>\n",
       "      <td>28</td>\n",
       "      <td>27.174672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1021445</td>\n",
       "      <td>29</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021445</td>\n",
       "      <td>46</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user  artist       count\n",
       "0  1021445      12  184.222707\n",
       "1  1021445      16    1.000000\n",
       "2  1021445      28   27.174672\n",
       "3  1021445      29    1.000000\n",
       "4  1021445      46    1.000000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(df_events['count'].min(), df_events['count'].max()))\n",
    "df_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df_events, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_random(testset, n=10):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r in testset:\n",
    "        if len(top_n[uid]) == 0:\n",
    "            for i in range(0, 10):\n",
    "                top_n[uid].append((rd.choice(item_dist.index), i))\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_mp(testset, n=10):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r in testset:\n",
    "        if len(top_n[uid]) == 0:\n",
    "            for iid, count in item_dist[:n].items():\n",
    "                top_n[uid].append((iid, count))\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae_of_groups(predictions) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    print('All: ')\n",
    "    mae_all: float = accuracy.mae(predictions)\n",
    "    low_predictions = []\n",
    "    med_predictions = []\n",
    "    high_predictions = []\n",
    "    for uid, iid, true_r, est, details in predictions:\n",
    "        prediction = [(uid, iid, true_r, est, details)]\n",
    "        if uid in low_users.index:\n",
    "            low_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "        elif uid in medium_users.index:\n",
    "            med_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "        else:\n",
    "            high_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "\n",
    "    mae_low: float = np.mean(low_predictions)\n",
    "    mae_med: float = np.mean(med_predictions)\n",
    "    mae_high: float = np.mean(high_predictions)\n",
    "    print('LowMS: ' + str(mae_low))\n",
    "    print('MedMS: ' + str(mae_med))\n",
    "    print('HighMS: ' + str(mae_high))\n",
    "    # print(stats.ttest_ind(low_predictions, high_predictions))\n",
    "\n",
    "    return (pd.DataFrame({'mae_all': [mae_all], 'mae_low': [mae_low], 'mae_med': [mae_med], \n",
    "    'mae_high': [mae_high]}), pd.DataFrame({'mae': low_predictions}), pd.DataFrame({'mae': med_predictions}), pd.DataFrame({'mae': high_predictions}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create item dataframe with normalized item counts\n",
    "df_item_dist = pd.DataFrame(item_dist)\n",
    "df_item_dist.columns = ['count']\n",
    "df_item_dist['count'] /= no_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:30: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:30: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:30: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:30: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\PREINS~1\\AppData\\Local\\Temp/ipykernel_27908/43353216.py:30: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if algo_names[i] is not 'Random' and algo_names[i] is not 'MostPopular':\n",
      "C:\\Users\\PREINS~1\\AppData\\Local\\Temp/ipykernel_27908/43353216.py:30: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if algo_names[i] is not 'Random' and algo_names[i] is not 'MostPopular':\n"
     ]
    }
   ],
   "source": [
    "def validate(trainset, testset) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    sim_users = {'name': 'cosine', 'user_based': True}  # compute cosine similarities between users\n",
    "    algos = [] # Random and MostPopular is calculated by default\n",
    "    algos.append(None)#Random())\n",
    "    algos.append(None)#MostPopular())\n",
    "    algos.append(BaselineOnly())\n",
    "    algos.append(KNNBasic(sim_options = sim_users, k=40))\n",
    "    algos.append(KNNWithMeans(sim_options = sim_users, k=40))\n",
    "    algos.append(NMF(n_factors = 15))\n",
    "    algo_names = ['Random',\n",
    "                'MostPopular',\n",
    "                'UserItemAvg',\n",
    "                'UserKNN',\n",
    "                'UserKNNAvg',\n",
    "                'NMF']\n",
    "\n",
    "    i = 0\n",
    "    performance_list: List[pd.DataFrame] = []\n",
    "    low_mae: List[pd.DataFrame] = []\n",
    "    med_mae: List[pd.DataFrame] = []\n",
    "    high_mae: List[pd.DataFrame] = []\n",
    "\n",
    "    for i in range(0, len(algo_names)):\n",
    "        df_item_dist[algo_names[i]] = 0\n",
    "        low_rec_gap = 0\n",
    "        medium_rec_gap = 0\n",
    "        high_rec_gap = 0\n",
    "\n",
    "        # get accuracy for personalized approaches\n",
    "        if algo_names[i] is not 'Random' and algo_names[i] is not 'MostPopular':\n",
    "            algos[i].fit(trainset)\n",
    "            predictions = algos[i].test(testset)\n",
    "            print(algo_names[i])\n",
    "            performance, low_performance, med_performance, high_performance = get_mae_of_groups(predictions)\n",
    "\n",
    "            performance[\"algo\"] = algo_names[i]\n",
    "            performance_list.append(performance)\n",
    "\n",
    "            low_performance[\"algo\"] = [algo_names[i]] * low_performance.shape[0]\n",
    "            low_mae.append(low_performance)\n",
    "\n",
    "            med_performance[\"algo\"] = [algo_names[i]] * med_performance.shape[0]\n",
    "            med_mae.append(med_performance)\n",
    "\n",
    "            high_performance[\"algo\"] = [algo_names[i]] * high_performance.shape[0]\n",
    "            high_mae.append(high_performance)\n",
    "\n",
    "    performance_data: pd.DataFrame = pd.concat(performance_list)\n",
    "    low_mae_data: pd.DataFrame = pd.concat(low_mae)\n",
    "    med_mae_data: pd.DataFrame = pd.concat(med_mae)\n",
    "    high_mae_data: pd.DataFrame = pd.concat(high_mae)\n",
    "\n",
    "    return (performance_data, low_mae_data, med_mae_data, high_mae_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "UserItemAvg\n",
      "All: \n",
      "MAE:  38.5612\n",
      "LowMS: 42.94802225638076\n",
      "MedMS: 33.90013072887102\n",
      "HighMS: 40.68639747115602\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UserKNN\n",
      "All: \n",
      "MAE:  45.6320\n",
      "LowMS: 49.75989995441734\n",
      "MedMS: 42.483604584035085\n",
      "HighMS: 45.99103663278319\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UserKNNAvg\n",
      "All: \n",
      "MAE:  41.8842\n",
      "LowMS: 46.58083982804533\n",
      "MedMS: 37.58534841057563\n",
      "HighMS: 43.2426341108826\n",
      "NMF\n",
      "All: \n",
      "MAE:  34.8523\n",
      "LowMS: 38.415371883576334\n",
      "MedMS: 30.62654021209281\n",
      "HighMS: 37.16152437455301\n"
     ]
    }
   ],
   "source": [
    "trainset, testset = train_test_split(data, test_size = 0.2, random_state = my_seed)\n",
    "\n",
    "performance_data, low_mae_data, med_mae_data, high_mae_data = validate(trainset=trainset, testset=testset)\n",
    "\n",
    "performance_data.to_csv(performance_data_file, encoding='utf-8')\n",
    "low_mae_data.to_csv(low_mae_data_file, encoding='utf-8')\n",
    "med_mae_data.to_csv(med_mae_data_file, encoding='utf-8')\n",
    "high_mae_data.to_csv(high_mae_data_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_data = pd.read_csv(performance_data_file, encoding='utf-8')\n",
    "low_mae_data = pd.read_csv(low_mae_data_file, encoding='utf-8')\n",
    "med_mae_data = pd.read_csv(med_mae_data_file, encoding='utf-8')\n",
    "high_mae_data = pd.read_csv(high_mae_data_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_low_high: List[float] = []\n",
    "t_score_low_high: List[float] = []\n",
    "\n",
    "for algo in low_mae_data['algo'].unique():\n",
    "    low_mae = low_mae_data[low_mae_data['algo'] == algo]['mae'].tolist()\n",
    "    high_mae = high_mae_data[high_mae_data['algo'] == algo]['mae'].tolist()\n",
    "    t_stat, p = stats.ttest_ind(low_mae, high_mae)\n",
    "\n",
    "    p_values_low_high.append(p)\n",
    "    t_score_low_high.append(t_stat)\n",
    "\n",
    "two_sample_ttest_data: pd.DataFrame = pd.DataFrame({'p_value': p_values_low_high, 't_score': t_score_low_high})\n",
    "\n",
    "two_sample_ttest_data.to_csv(two_sample_ttest_data_file, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:0\n",
      "Estimating biases using als...\n",
      "UserItemAvg\n",
      "All: \n",
      "MAE:  38.5778\n",
      "LowMS: 43.20939266538006\n",
      "MedMS: 34.009743511116035\n",
      "HighMS: 40.354254467042935\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UserKNN\n",
      "All: \n",
      "MAE:  45.6260\n",
      "LowMS: 49.845253223588436\n",
      "MedMS: 42.7074971216775\n",
      "HighMS: 45.593557982682206\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UserKNNAvg\n",
      "All: \n",
      "MAE:  41.8959\n",
      "LowMS: 46.79212662418838\n",
      "MedMS: 37.696166555642954\n",
      "HighMS: 42.93792752845092\n",
      "NMF\n",
      "All: \n",
      "MAE:  34.7358\n",
      "LowMS: 38.47888991164326\n",
      "MedMS: 30.61361159908839\n",
      "HighMS: 36.742829501939234\n",
      "fold:1\n",
      "Estimating biases using als...\n",
      "UserItemAvg\n",
      "All: \n",
      "MAE:  38.6710\n",
      "LowMS: 42.95710287284193\n",
      "MedMS: 34.30439121452419\n",
      "HighMS: 40.47191653816627\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UserKNN\n",
      "All: \n",
      "MAE:  45.7612\n",
      "LowMS: 49.5074822479507\n",
      "MedMS: 43.0304446382831\n",
      "HighMS: 45.89593262230686\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UserKNNAvg\n",
      "All: \n",
      "MAE:  42.0610\n",
      "LowMS: 46.47743093804184\n",
      "MedMS: 38.147868477842486\n",
      "HighMS: 43.13953971167194\n",
      "NMF\n",
      "All: \n",
      "MAE:  34.9436\n",
      "LowMS: 38.21637334008859\n",
      "MedMS: 31.038575688254877\n",
      "HighMS: 37.07526087255139\n",
      "fold:2\n",
      "Estimating biases using als...\n",
      "UserItemAvg\n",
      "All: \n",
      "MAE:  38.6267\n",
      "LowMS: 42.692667077739586\n",
      "MedMS: 34.31833648949957\n",
      "HighMS: 40.56366238962685\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UserKNN\n",
      "All: \n",
      "MAE:  45.6553\n",
      "LowMS: 49.011270997458254\n",
      "MedMS: 43.04412951514622\n",
      "HighMS: 46.00406949196721\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UserKNNAvg\n",
      "All: \n",
      "MAE:  41.9465\n",
      "LowMS: 46.14254573169047\n",
      "MedMS: 38.03649778859482\n",
      "HighMS: 43.23598864717305\n",
      "NMF\n",
      "All: \n",
      "MAE:  34.8500\n",
      "LowMS: 37.88850601331304\n",
      "MedMS: 30.9257067718244\n",
      "HighMS: 37.22976614426113\n",
      "fold:3\n",
      "Estimating biases using als...\n",
      "UserItemAvg\n",
      "All: \n",
      "MAE:  38.5521\n",
      "LowMS: 42.70055205783307\n",
      "MedMS: 34.40810435011609\n",
      "HighMS: 40.21612054591898\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UserKNN\n",
      "All: \n",
      "MAE:  45.6780\n",
      "LowMS: 49.33140695824935\n",
      "MedMS: 43.23637539457013\n",
      "HighMS: 45.5412103283541\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UserKNNAvg\n",
      "All: \n",
      "MAE:  41.9698\n",
      "LowMS: 46.43122563519757\n",
      "MedMS: 38.237155789943785\n",
      "HighMS: 42.79888958470275\n",
      "NMF\n",
      "All: \n",
      "MAE:  34.7865\n",
      "LowMS: 37.81465007812977\n",
      "MedMS: 31.262156240181618\n",
      "HighMS: 36.66369556376425\n",
      "fold:4\n",
      "Estimating biases using als...\n",
      "UserItemAvg\n",
      "All: \n",
      "MAE:  38.5613\n",
      "LowMS: 42.948022268925456\n",
      "MedMS: 33.90013051316954\n",
      "HighMS: 40.686739020428156\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UserKNN\n",
      "All: \n",
      "MAE:  45.6321\n",
      "LowMS: 49.75989154196364\n",
      "MedMS: 42.48361721383677\n",
      "HighMS: 45.991323424057924\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UserKNNAvg\n",
      "All: \n",
      "MAE:  41.8843\n",
      "LowMS: 46.58083780627281\n",
      "MedMS: 37.585356155384275\n",
      "HighMS: 43.24306811502281\n",
      "NMF\n",
      "All: \n",
      "MAE:  34.8499\n",
      "LowMS: 38.37779807792674\n",
      "MedMS: 30.622621230847514\n",
      "HighMS: 37.19367644871763\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=my_seed)\n",
    "performance_data_cv_list: List[pd.DataFrame] = []\n",
    "low_mae_data_cv_list: List[pd.DataFrame] = []\n",
    "med_mae_data_cv_list: List[pd.DataFrame] = []\n",
    "high_mae_data_cv_list: List[pd.DataFrame] = []\n",
    "\n",
    "for idx, (trainset_cv, testset_cv) in enumerate(kf.split(data)):\n",
    "    print('fold:' + str(idx))\n",
    "    performance_data_cv, low_mae_data_cv, med_mae_data_cv, high_mae_data_cv = validate(trainset=trainset_cv, testset=testset_cv)\n",
    "\n",
    "    performance_data_cv[\"fold\"] = idx\n",
    "    low_mae_data_cv[\"fold\"] = [idx] * low_mae_data_cv.shape[0]\n",
    "    med_mae_data_cv[\"fold\"] = [idx] * med_mae_data_cv.shape[0]\n",
    "    high_mae_data_cv[\"fold\"] = [idx] * high_mae_data_cv.shape[0]\n",
    "\n",
    "    performance_data_cv_list.append(performance_data_cv)\n",
    "    low_mae_data_cv_list.append(low_mae_data_cv)\n",
    "    med_mae_data_cv_list.append(med_mae_data_cv)\n",
    "    high_mae_data_cv_list.append(high_mae_data_cv)\n",
    "    \n",
    "performance_data_cv_data: pd.DataFrame = pd.concat(performance_data_cv_list)\n",
    "low_mae_data_cv_data: pd.DataFrame = pd.concat(low_mae_data_cv_list)\n",
    "med_mae_data_cv_data: pd.DataFrame = pd.concat(med_mae_data_cv_list)\n",
    "high_mae_data_cv_data: pd.DataFrame = pd.concat(high_mae_data_cv_list)\n",
    "\n",
    "performance_data_cv_data.to_csv(performance_data_cv_file, encoding='utf-8')\n",
    "low_mae_data_cv_data.to_csv(low_mae_data_cv_file, encoding='utf-8')\n",
    "med_mae_data_cv_data.to_csv(med_mae_data_cv_file, encoding='utf-8')\n",
    "high_mae_data_cv_data.to_csv(high_mae_data_cv_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_data_cv_data = pd.read_csv(performance_data_cv_file, encoding='utf-8')\n",
    "low_mae_data_cv_data = pd.read_csv(low_mae_data_cv_file, encoding='utf-8')\n",
    "med_mae_data_cv_data = pd.read_csv(med_mae_data_cv_file, encoding='utf-8')\n",
    "high_mae_data_cv_data = pd.read_csv(high_mae_data_cv_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c627be1547c7c874688ddee1aaaa67b367275f1752d282bff5eb427acd241334"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
